package com.example.emotionlink.ViewModel

import android.Manifest
import android.app.Application
import android.media.AudioAttributes
import android.media.AudioFormat
import android.media.AudioManager
import android.media.AudioRecord
import android.media.MediaPlayer
import android.media.MediaRecorder
import android.os.Handler
import android.os.Looper
import androidx.annotation.RequiresPermission
import androidx.lifecycle.AndroidViewModel
import androidx.lifecycle.viewModelScope
import com.example.emotionlink.Utils.LogUtils
import com.example.emotionlink.WebSocket.CallMessageCallback
import com.example.emotionlink.WebSocket.MessageCallback
import com.example.emotionlink.WebSocket.WebSocketStatusListener
import com.example.emotionlink.WebSocket.WebsocketUploaderManager
import kotlinx.coroutines.CoroutineScope
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.Job
import kotlinx.coroutines.flow.MutableStateFlow
import kotlinx.coroutines.flow.StateFlow
import kotlinx.coroutines.flow.asStateFlow
import kotlinx.coroutines.launch
import org.json.JSONObject
import java.util.concurrent.atomic.AtomicBoolean
import kotlin.math.abs

class VoiceCallViewModel(application: Application) : AndroidViewModel(application) {
    private val TAG = "VoiceCallViewModel"
    private val audioManager = application.getSystemService(Application.AUDIO_SERVICE) as AudioManager
    private val mainHandler = Handler(Looper.getMainLooper())

    private var audioRecord: AudioRecord? = null
    private var mediaPlayer: MediaPlayer? = null
    private var isRecording = AtomicBoolean(false)
    private var isPlaying = AtomicBoolean(false)
    private var recordingJob: Job? = null
    
    // VADç›¸å…³å‚æ•°
    private val SILENCE_THRESHOLD = 400 // é™éŸ³é˜ˆå€¼ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    private val SILENCE_DURATION = 1000L // é™éŸ³æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    private var lastVoiceTime = 0L // ä¸Šæ¬¡æ£€æµ‹åˆ°å£°éŸ³çš„æ—¶é—´
    private var isVoiceActive = false // æ˜¯å¦æ­£åœ¨è¯´è¯
    
    private var remoteUserId: String? = null
    private var callId: String? = null
    private var _language: String=""
    private val _currentTargetUser = MutableStateFlow<String?>(null)
    val currentTargetUser: StateFlow<String?> = _currentTargetUser
    //ç›´æ¥ä»overlayViewModelé‡Œè·å¾—wsClientå®ä¾‹
    var overlayViewModel: OverlayViewModel? = null
    fun bindOverlayViewModel(vm: OverlayViewModel) {
        overlayViewModel = vm
    }
    private var wsClient = overlayViewModel?.wsClient

    fun setTargetUser(userId: String) {
        _currentTargetUser.value = userId
    }

    fun setCurrentLanguage(language: String) {
        _language = language
    }
    // çŠ¶æ€æµ
    private val _callState = MutableStateFlow(CallState.IDLE)
    val callState: StateFlow<CallState> = _callState.asStateFlow()
    
    private val _isMuted = MutableStateFlow(false)
    val isMuted: StateFlow<Boolean> = _isMuted.asStateFlow()
    
    private val _isSpeakerOn = MutableStateFlow(true)
    val isSpeakerOn: StateFlow<Boolean> = _isSpeakerOn.asStateFlow()

    private val _incomingCall = MutableStateFlow<String?>(null) // userId æˆ– null
    val incomingCall: StateFlow<String?> = _incomingCall.asStateFlow()

    fun notifyIncomingCall(fromUserId: String) {
        _incomingCall.value = fromUserId
    }

    enum class CallState {
        IDLE,
        CALLING,
        CONNECTED,
        ENDED
    }

    init {
        LogUtils.d(TAG,"$TAG init")
        WebsocketUploaderManager.setCallMessageCallback(object : CallMessageCallback {
            override fun onCallMessageReceived(
                fromLang: String,
                toLang: String,
                text: String,
                wavUrl: String,
                duration: String
            ) {
//                LogUtils.d(TAG,  "æ¥è‡ª$fromLang,ç›®æ ‡$toLang,æ–‡å­—$text,æ˜¯å¦ç›¸ç­‰${text == wavUrl},"
//                        + "éŸ³é¢‘åœ°å€: $wavUrl,æŒç»­æ—¶é—´$duration\"")
                startPlaying()
//                onAudioDataReceived(wavUrl)
                wsClient?.sendInit()
            }

            @RequiresPermission(Manifest.permission.RECORD_AUDIO)
            override fun onIncomingCall(fromUserId: String) {
                LogUtils.d(TAG, "ğŸ“ æ¥ç”µï¼š$fromUserId")
                notifyIncomingCall(fromUserId)
                acceptCall()
            }

            override fun onError(e: Exception) {
            }
        })
    }

    @RequiresPermission(Manifest.permission.RECORD_AUDIO)
    fun startCall() {
        if (isRecording.get() || isPlaying.get()) return
        LogUtils.d(TAG,_language)
//        wsClient?.sendInit()
//        wsClient?.sendCallRequest(_language,remoteUserId)
        if (wsClient == null) {
            LogUtils.d(TAG, "WebSocketæœªåˆå§‹åŒ–ï¼Œæ­£åœ¨åˆå§‹åŒ–...")
            // åˆå§‹åŒ–WebSocket
            WebsocketUploaderManager.initUploader(_language, object : WebSocketStatusListener {
                @RequiresPermission(Manifest.permission.RECORD_AUDIO)
                override fun onConnected() {
                    LogUtils.d(TAG, "WebSocketè¿æ¥æˆåŠŸï¼Œå¼€å§‹é€šè¯")
                    wsClient = WebsocketUploaderManager.getUploader()
                    wsClient?.sendCallRequest(_language)
                    wsClient?.sendInit()
                }

                override fun onError(e: Exception) {
                    LogUtils.e(TAG, "WebSocketè¿æ¥å¤±è´¥: ${e.message}")
                    updateCallState(CallState.ENDED)
                }
            })
            return
        }
        initializeAudio()
        startRecording()
        updateCallState(CallState.CALLING)
    }
    
    @RequiresPermission(Manifest.permission.RECORD_AUDIO)
    fun acceptCall() {
        if (isRecording.get() || isPlaying.get()) return

        wsClient?.sendAcceptRequest(_language)//ä»£æ”¹ï¼Œåº”è¯¥ç”¨å›è°ƒæ¥æ˜¾ç¤ºè°æ‹¨æ‰“çš„ç”µè¯

        initializeAudio()
        startRecording()
        updateCallState(CallState.CONNECTED)
    }
    
    fun rejectCall() {
        if (_currentTargetUser.value == null||_language.isEmpty()) return
        

        wsClient?.sendReject(_language)//ä»£æ”¹
        updateCallState(CallState.ENDED)
    }
    
    fun endCall() {
        if (_currentTargetUser.value != null && _language.isNotEmpty()) {
            wsClient?.sendCallEnd(_language)//ä»£æ”¹
        }
        stopRecording()
        stopPlaying()
        releaseResources()
        updateCallState(CallState.ENDED)
    }
    
    fun toggleMute() {
        _isMuted.value = !_isMuted.value
        audioRecord?.let { record ->
            if (_isMuted.value) {
                record.stop()
            } else {
                record.startRecording()
            }
        }
    }
    
    fun toggleSpeaker() {
        _isSpeakerOn.value = !_isSpeakerOn.value
        audioManager.isSpeakerphoneOn = _isSpeakerOn.value
    }

    //è®¾ç½®éŸ³é¢‘æ’­æ”¾æ¨¡å¼ã€‚åŒå‘ä¼ è¾“
    private fun initializeAudio() {
        audioManager.mode = AudioManager.MODE_IN_COMMUNICATION
        audioManager.isSpeakerphoneOn = _isSpeakerOn.value
    }

    //å¼€å§‹å½•éŸ³
    @RequiresPermission(Manifest.permission.RECORD_AUDIO)
    private fun startRecording() {
        if (isRecording.get()) return
        
        val minBufferSize = AudioRecord.getMinBufferSize(
            16000,
            AudioFormat.CHANNEL_IN_MONO,
            AudioFormat.ENCODING_PCM_16BIT
        )*10
        
        audioRecord = AudioRecord(
            MediaRecorder.AudioSource.VOICE_COMMUNICATION,
            16000,
            AudioFormat.CHANNEL_IN_MONO,
            AudioFormat.ENCODING_PCM_16BIT,
            minBufferSize
        )
        
        isRecording.set(true)
        LogUtils.d(TAG,"å¼€å§‹å½•éŸ³")
        recordingJob = CoroutineScope(Dispatchers.IO).launch {
            val buffer = ByteArray(minBufferSize)
            audioRecord?.startRecording()
            
            while (isRecording.get()) {
                val readSize = audioRecord?.read(buffer, 0, buffer.size) ?: 0
                if (readSize > 0) {
                    // æ£€æµ‹æ˜¯å¦æœ‰å£°éŸ³
                    val hasVoice = detectVoice(buffer, readSize)
                    val currentTime = System.currentTimeMillis()
                    
                    if (hasVoice) {
                        lastVoiceTime = currentTime
                        if (!isVoiceActive) {
                            // å¼€å§‹æ–°çš„è¯­éŸ³æ®µ
                            isVoiceActive = true
                            LogUtils.d(TAG, "æ£€æµ‹åˆ°å£°éŸ³ï¼Œå¼€å§‹å‘é€éŸ³é¢‘")
                            wsClient?.sendInit()
                        }
                        // å‘é€éŸ³é¢‘æ•°æ®
                        wsClient?.sendAudioChunk(buffer, readSize)
                    } else if (isVoiceActive && (currentTime - lastVoiceTime > SILENCE_DURATION)) {
                        // é™éŸ³è¶…è¿‡é˜ˆå€¼ï¼Œç»“æŸå½“å‰è¯­éŸ³æ®µ
                        isVoiceActive = false
                        LogUtils.d(TAG, "æ£€æµ‹åˆ°é™éŸ³ï¼Œåœæ­¢å‘é€éŸ³é¢‘")
                        wsClient?.sendEnd("1")
                    }
                }
            }
        }
    }
    
    private fun startPlaying() {
        if (isPlaying.get()) return
        isPlaying.set(true)
    }
    
    private fun stopRecording() {
        isRecording.set(false)
        recordingJob?.cancel()
        audioRecord?.stop()
        audioRecord?.release()
        audioRecord = null
    }
    
    private fun stopPlaying() {
        isPlaying.set(false)
        mediaPlayer?.stop()
        mediaPlayer?.release()
        mediaPlayer = null
    }
    
    private fun releaseResources() {
        audioManager.mode = AudioManager.MODE_NORMAL
        audioManager.isSpeakerphoneOn = false
        callId = null
        remoteUserId = null
    }
    
    private fun updateCallState(state: CallState) {
        mainHandler.post {
            _callState.value = state
        }
    }
    
    private fun onAudioDataReceived(wavUrl: String) {
        try {
            LogUtils.d(TAG, "æ’­æ”¾éŸ³é¢‘")
            mediaPlayer?.release()
            mediaPlayer = MediaPlayer().apply {
                setAudioAttributes(
                    AudioAttributes.Builder()
                        .setUsage(AudioAttributes.USAGE_VOICE_COMMUNICATION)
                        .setContentType(AudioAttributes.CONTENT_TYPE_SPEECH)
                        .build()
                )
                setDataSource(wavUrl)
                setOnPreparedListener {
                    LogUtils.d(TAG, "MediaPlayer å‡†å¤‡å®Œæ¯•ï¼Œå¼€å§‹æ’­æ”¾")
                    it.start()
                }

                setOnErrorListener { mp, what, extra ->
                    LogUtils.e(TAG, "MediaPlayer æ’­æ”¾å‡ºé”™ what=$what extra=$extra")
                    true
                }
                prepareAsync()
            }
        } catch (e: Exception) {
            LogUtils.e(TAG, "æ’­æ”¾éŸ³é¢‘å¤±è´¥: ${e.message}")
        }
    }
    
    // æ£€æµ‹æ˜¯å¦æœ‰å£°éŸ³
    private fun detectVoice(buffer: ByteArray, size: Int): Boolean {
        var sum = 0.0
        // å°†å­—èŠ‚æ•°ç»„è½¬æ¢ä¸ºçŸ­æ•´å‹æ•°ç»„ï¼ˆ16ä½éŸ³é¢‘ï¼‰
        val shorts = ShortArray(size / 2)
        for (i in 0 until size / 2) {
            shorts[i] = (buffer[i * 2].toInt() and 0xFF or (buffer[i * 2 + 1].toInt() shl 8)).toShort()
        }
        
        // è®¡ç®—éŸ³é¢‘æ•°æ®çš„èƒ½é‡
        for (i in shorts.indices) {
            sum += abs(shorts[i].toDouble())
        }
        val average = sum / shorts.size
        
        return average > SILENCE_THRESHOLD
    }
    
    override fun onCleared() {
        super.onCleared()
        endCall()
    }
} 